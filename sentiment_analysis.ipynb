{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Sentiment Analysis\n",
    "\n",
    "In the previous notebook, we managed to achieve a decent test accuracy of ~85% using all of the common techniques used for sentiment analysis. In this notebook, we'll implement a model that achieves comparable results a lot faster. More specifically, we'll be implementing the \"FastText\" model from the paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759).\n",
    "\n",
    "\n",
    "This will allow us to achieve the same ~85% test accuracy as the last model, but much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "One of the key concepts in the FastText paper is that they calculate the n-grams of an input sentence and append them to the end of a sentence. Here, we'll use bi-grams. Briefly, a bi-gram is a pair of words/tokens that appear consecutively within a sentence. \n",
    "\n",
    "For example, in the sentence \"how are you ?\", the bi-grams are: \"how are\", \"are you\" and \"you ?\".\n",
    "\n",
    "The `generate_bigrams` function takes a sentence that has already been tokenized, calculates the bi-grams and appends them to the end of the tokenized list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109952e50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the libraries\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'film', 'is', 'terrible', 'This film', 'film is', 'is terrible']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigrams(['This', 'film', 'is', 'terrible'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText `Field`s have a `preprocessing` argument. A function passed here will be applied to a sentence after it has been tokenized (transformed from a string into a list of tokens), but before it has been indexed (transformed from a token to an integer). Here, we pass our `generate_bigrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', preprocessing=generate_bigrams)\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we load the IMDb dataset and create the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train, valid = train.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocab and load the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x118187840>, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, max_size=40000, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "This model has far fewer parameters than the previous model as it only has 2 layers that have any parameters, the embedding layer and the linear layer. There is no RNN component in sight!\n",
    "\n",
    "Instead, it first calculates the word embedding for each word using the `Embedding` layer, then calculates the average of all of the word embeddings and feeds this through the `Linear` layer, and that's it!\n",
    "\n",
    "![](https://i.imgur.com/e0sWZoZ.png)\n",
    "\n",
    "We implement the averaging with the `avg_pool2d` (average pool 2-dimensions) function. Initially, you may think using a 2-dimensional pooling seems strange, surely our sentences are 1-dimensional, not 2-dimensional? However, you can think of the word embeddings as a 2-dimensional grid, where the ones are along one axis and the dimensions of the word embeddings are along another. In the image below is an example sentence after being converted into 5-dimensional word embeddings, with the words along the vertical axis and the embeddings along the horizontal axis.\n",
    "\n",
    "![](https://i.imgur.com/SSH25NT.png)\n",
    "\n",
    "The `avg_pool2d` passes a filter of size `embedded.shape[1]` (i.e. the length of the sentence) by 1. This is shown in pink in the image below.\n",
    "\n",
    "![](https://i.imgur.com/U7eRnIe.png)\n",
    "\n",
    "The average value of all of the dimensions is calculated and concatenated into a 5-dimensional (in our pictoral examples, 100-dimensional in the code) tensor for each sentence. This tensor is then passed through the linear layer to produce our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "                \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, we'll create an instance of our `FastText` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And copy the pre-trained vectors to our embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.9409, -0.1506,  0.5157,  ...,  0.2661, -0.6054, -0.1816],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is the exact same as last time.\n",
    "\n",
    "We initialize our optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the criterion and place the model and criterion on the GPU (if available)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function to calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for training our model...\n",
    "\n",
    "**Note**: we are no longer using dropout so we do not need to use `model.train()`, but as mentioned in the 1st notebook, it is good practice to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for testing our model...\n",
    "\n",
    "**Note**: again, we leave `model.eval()` even though we do not use dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siqi/miniconda3/envs/siqi_tdi/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.687, Train Acc: 57.01%, Val. Loss: 0.634963, Val. Acc: 70.51%\n",
      "Epoch: 02, Train Loss: 0.646, Train Acc: 71.90%, Val. Loss: 0.503981, Val. Acc: 76.01%\n",
      "Epoch: 03, Train Loss: 0.565, Train Acc: 79.38%, Val. Loss: 0.433833, Val. Acc: 80.30%\n",
      "Epoch: 04, Train Loss: 0.484, Train Acc: 83.89%, Val. Loss: 0.399229, Val. Acc: 83.68%\n",
      "Epoch: 05, Train Loss: 0.416, Train Acc: 87.39%, Val. Loss: 0.392316, Val. Acc: 85.91%\n",
      "Epoch: 06, Train Loss: 0.365, Train Acc: 88.92%, Val. Loss: 0.391880, Val. Acc: 86.98%\n",
      "Epoch: 07, Train Loss: 0.324, Train Acc: 90.42%, Val. Loss: 0.403797, Val. Acc: 87.77%\n",
      "Epoch: 08, Train Loss: 0.289, Train Acc: 91.44%, Val. Loss: 0.419661, Val. Acc: 88.22%\n",
      "Epoch: 09, Train Loss: 0.262, Train Acc: 92.29%, Val. Loss: 0.438539, Val. Acc: 88.67%\n",
      "Epoch: 10, Train Loss: 0.241, Train Acc: 92.91%, Val. Loss: 0.457921, Val. Acc: 88.87%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, optimizer, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and get the test accuracy!\n",
    "\n",
    "The results are comparable to the results in the last notebook, but training takes considerably less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siqi/miniconda3/envs/siqi_tdi/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.446, Test Acc: 88.74%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iter, optimizer, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Reload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saved_model_state.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9f4354aba11c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mINPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "                \n",
    "        return self.fc(pooled)\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "    \n",
    "model_reload = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)\n",
    "model_reload.load_state_dict(torch.load('saved_model_state.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "And as before, we can test on any input the user provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(article):\n",
    "    article=' '.join(article)\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(article)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = F.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = F.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001665666641201824"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"House bill is detrimental to the public and needs to get rid of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2817367963281601e-20"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis For all news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "      <td>Politics*0.45669132</td>\n",
       "      <td>Health Care*0.22221036</td>\n",
       "      <td>Court/Legal System*0.08490019</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "      <td>Crime*0.30302584</td>\n",
       "      <td>Unlabeled*0.28304937</td>\n",
       "      <td>Crime*0.09784402</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "      <td>Unlabeled*0.19544908</td>\n",
       "      <td>Unlabeled*0.14889707</td>\n",
       "      <td>Religion*0.097971134</td>\n",
       "      <td>Religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "      <td>Unlabeled*0.2989482</td>\n",
       "      <td>Religion*0.1702237</td>\n",
       "      <td>Gender Issues*0.11686472</td>\n",
       "      <td>Religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "      <td>Foreign Policy/Korea*0.61946124</td>\n",
       "      <td>Politics*0.19456775</td>\n",
       "      <td>Foreign Policy/Trade*0.09577953</td>\n",
       "      <td>Foreign Policy/Korea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title     publication  \\\n",
       "0  17283  House Republicans Fret About Winning Their Hea...  New York Times   \n",
       "1  17284  Rift Between Officers and Residents as Killing...  New York Times   \n",
       "2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...  New York Times   \n",
       "3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...  New York Times   \n",
       "4  17287  Kim Jong-un Says North Korea Is Preparing to T...  New York Times   \n",
       "\n",
       "                          author        date    year  month  url  \\\n",
       "0                     Carl Hulse  2016-12-31  2016.0   12.0  NaN   \n",
       "1  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0  NaN   \n",
       "2                   Margalit Fox  2017-01-06  2017.0    1.0  NaN   \n",
       "3               William McDonald  2017-04-10  2017.0    4.0  NaN   \n",
       "4                  Choe Sang-Hun  2017-01-02  2017.0    1.0  NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  WASHINGTON  —   Congressional Republicans have...   \n",
       "1  After the bullet shells get counted, the blood...   \n",
       "2  When Walt Disney’s “Bambi” opened in 1942, cri...   \n",
       "3  Death may be the great equalizer, but it isn’t...   \n",
       "4  SEOUL, South Korea  —   North Korea’s leader, ...   \n",
       "\n",
       "                            topic1                  topic2  \\\n",
       "0              Politics*0.45669132  Health Care*0.22221036   \n",
       "1                 Crime*0.30302584    Unlabeled*0.28304937   \n",
       "2             Unlabeled*0.19544908    Unlabeled*0.14889707   \n",
       "3              Unlabeled*0.2989482      Religion*0.1702237   \n",
       "4  Foreign Policy/Korea*0.61946124     Politics*0.19456775   \n",
       "\n",
       "                            topic3                 topic  \n",
       "0    Court/Legal System*0.08490019              Politics  \n",
       "1                 Crime*0.09784402                 Crime  \n",
       "2             Religion*0.097971134              Religion  \n",
       "3         Gender Issues*0.11686472              Religion  \n",
       "4  Foreign Policy/Trade*0.09577953  Foreign Policy/Korea  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news= dill.load(open('../data/article_set', 'rb'))\n",
    "all_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(article, tokenizer, stop, punct):\n",
    "    \n",
    "    article = article.lower()  # Convert to lowercase.\n",
    "    \n",
    "    article = tokenizer.tokenize(article)  # Split into words.\n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    article = [token for token in article if not token.isdigit()]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    article = [token for token in article if len(token) > 3]\n",
    "    \n",
    "    # Remove stop-words\n",
    "    article = [token for token in article if token not in stop]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    article = [token for token in article if token not in punct]\n",
    "    \n",
    "    return article\n",
    "    \n",
    "def get_sentiment(article,tokenizer, stop, punct):\n",
    "    \n",
    "    article=clean_text(article, tokenizer, stop, punct)\n",
    "    \n",
    "    #print (article)\n",
    "    \n",
    "    if len(article)>5:\n",
    "        return predict_sentiment(article)\n",
    "    else:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "# create English stop words list\n",
    "en_stop = stopwords.words('english')\n",
    "# custom stop words\n",
    "cus_stop=['mr','mrs','ms','said','dr']\n",
    "# finalize stop words\n",
    "stop=en_stop+cus_stop\n",
    "    \n",
    "# punctuation characters\n",
    "punct=set(string.punctuation) \n",
    "\n",
    "senti_score=[]\n",
    "articles=all_news['content']\n",
    "\n",
    "for article in articles:\n",
    "    senti_score.append(get_sentiment(article,tokenizer, stop, punct))\n",
    "\n",
    "all_news['sentiment']=np.array(senti_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "      <td>Politics*0.45669132</td>\n",
       "      <td>Health Care*0.22221036</td>\n",
       "      <td>Court/Legal System*0.08490019</td>\n",
       "      <td>Politics</td>\n",
       "      <td>9.950062e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "      <td>Crime*0.30302584</td>\n",
       "      <td>Unlabeled*0.28304937</td>\n",
       "      <td>Crime*0.09784402</td>\n",
       "      <td>Crime</td>\n",
       "      <td>4.581105e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "      <td>Unlabeled*0.19544908</td>\n",
       "      <td>Unlabeled*0.14889707</td>\n",
       "      <td>Religion*0.097971134</td>\n",
       "      <td>Religion</td>\n",
       "      <td>1.965388e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "      <td>Unlabeled*0.2989482</td>\n",
       "      <td>Religion*0.1702237</td>\n",
       "      <td>Gender Issues*0.11686472</td>\n",
       "      <td>Religion</td>\n",
       "      <td>3.980621e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "      <td>Foreign Policy/Korea*0.61946124</td>\n",
       "      <td>Politics*0.19456775</td>\n",
       "      <td>Foreign Policy/Trade*0.09577953</td>\n",
       "      <td>Foreign Policy/Korea</td>\n",
       "      <td>7.840580e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17288</td>\n",
       "      <td>Sick With a Cold, Queen Elizabeth Misses New Y...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Sewell Chan</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LONDON  —   Queen Elizabeth II, who has been b...</td>\n",
       "      <td>Unlabeled*0.29382992</td>\n",
       "      <td>Entertainment*0.19128913</td>\n",
       "      <td>Sports*0.17322288</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1.589564e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17289</td>\n",
       "      <td>Taiwan’s President Accuses China of Renewed In...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Javier C. Hernández</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BEIJING  —   President Tsai   of Taiwan sharpl...</td>\n",
       "      <td>Foreign Policy/China*0.5561201</td>\n",
       "      <td>Foreign Policy/Trade*0.1705865</td>\n",
       "      <td>Politics*0.06863873</td>\n",
       "      <td>Foreign Policy/China</td>\n",
       "      <td>4.836930e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17290</td>\n",
       "      <td>After ‘The Biggest Loser,’ Their Bodies Fought...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Gina Kolata</td>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Danny Cahill stood, slightly dazed, in a blizz...</td>\n",
       "      <td>Scientific Research*0.23732938</td>\n",
       "      <td>Unlabeled*0.14938736</td>\n",
       "      <td>Domestic Affairs*0.111396484</td>\n",
       "      <td>Scientific Research</td>\n",
       "      <td>9.918941e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17291</td>\n",
       "      <td>First, a Mixtape. Then a Romance. - The New Yo...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Katherine Rosman</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just how   is Hillary Kerr, the    founder of ...</td>\n",
       "      <td>Unlabeled*0.3029118</td>\n",
       "      <td>Gender Issues*0.22602034</td>\n",
       "      <td>Food/Lifestyle*0.082700245</td>\n",
       "      <td>Gender Issues</td>\n",
       "      <td>1.434017e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17292</td>\n",
       "      <td>Calling on Angels While Enduring the Trials of...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Angels are everywhere in the Muñiz family’s ap...</td>\n",
       "      <td>Unlabeled*0.69932485</td>\n",
       "      <td>Domestic Affairs*0.080745846</td>\n",
       "      <td>Education*0.05090855</td>\n",
       "      <td>Domestic Affairs</td>\n",
       "      <td>3.759886e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title     publication  \\\n",
       "0  17283  House Republicans Fret About Winning Their Hea...  New York Times   \n",
       "1  17284  Rift Between Officers and Residents as Killing...  New York Times   \n",
       "2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...  New York Times   \n",
       "3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...  New York Times   \n",
       "4  17287  Kim Jong-un Says North Korea Is Preparing to T...  New York Times   \n",
       "5  17288  Sick With a Cold, Queen Elizabeth Misses New Y...  New York Times   \n",
       "6  17289  Taiwan’s President Accuses China of Renewed In...  New York Times   \n",
       "7  17290  After ‘The Biggest Loser,’ Their Bodies Fought...  New York Times   \n",
       "8  17291  First, a Mixtape. Then a Romance. - The New Yo...  New York Times   \n",
       "9  17292  Calling on Angels While Enduring the Trials of...  New York Times   \n",
       "\n",
       "                          author        date    year  month  url  \\\n",
       "0                     Carl Hulse  2016-12-31  2016.0   12.0  NaN   \n",
       "1  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0  NaN   \n",
       "2                   Margalit Fox  2017-01-06  2017.0    1.0  NaN   \n",
       "3               William McDonald  2017-04-10  2017.0    4.0  NaN   \n",
       "4                  Choe Sang-Hun  2017-01-02  2017.0    1.0  NaN   \n",
       "5                    Sewell Chan  2017-01-02  2017.0    1.0  NaN   \n",
       "6            Javier C. Hernández  2017-01-02  2017.0    1.0  NaN   \n",
       "7                    Gina Kolata  2017-02-08  2017.0    2.0  NaN   \n",
       "8               Katherine Rosman  2016-12-31  2016.0   12.0  NaN   \n",
       "9                    Andy Newman  2016-12-31  2016.0   12.0  NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  WASHINGTON  —   Congressional Republicans have...   \n",
       "1  After the bullet shells get counted, the blood...   \n",
       "2  When Walt Disney’s “Bambi” opened in 1942, cri...   \n",
       "3  Death may be the great equalizer, but it isn’t...   \n",
       "4  SEOUL, South Korea  —   North Korea’s leader, ...   \n",
       "5  LONDON  —   Queen Elizabeth II, who has been b...   \n",
       "6  BEIJING  —   President Tsai   of Taiwan sharpl...   \n",
       "7  Danny Cahill stood, slightly dazed, in a blizz...   \n",
       "8  Just how   is Hillary Kerr, the    founder of ...   \n",
       "9  Angels are everywhere in the Muñiz family’s ap...   \n",
       "\n",
       "                            topic1                          topic2  \\\n",
       "0              Politics*0.45669132          Health Care*0.22221036   \n",
       "1                 Crime*0.30302584            Unlabeled*0.28304937   \n",
       "2             Unlabeled*0.19544908            Unlabeled*0.14889707   \n",
       "3              Unlabeled*0.2989482              Religion*0.1702237   \n",
       "4  Foreign Policy/Korea*0.61946124             Politics*0.19456775   \n",
       "5             Unlabeled*0.29382992        Entertainment*0.19128913   \n",
       "6   Foreign Policy/China*0.5561201  Foreign Policy/Trade*0.1705865   \n",
       "7   Scientific Research*0.23732938            Unlabeled*0.14938736   \n",
       "8              Unlabeled*0.3029118        Gender Issues*0.22602034   \n",
       "9             Unlabeled*0.69932485    Domestic Affairs*0.080745846   \n",
       "\n",
       "                            topic3                 topic     sentiment  \n",
       "0    Court/Legal System*0.08490019              Politics  9.950062e-01  \n",
       "1                 Crime*0.09784402                 Crime  4.581105e-01  \n",
       "2             Religion*0.097971134              Religion  1.965388e-03  \n",
       "3         Gender Issues*0.11686472              Religion  3.980621e-03  \n",
       "4  Foreign Policy/Trade*0.09577953  Foreign Policy/Korea  7.840580e-03  \n",
       "5                Sports*0.17322288         Entertainment  1.589564e-10  \n",
       "6              Politics*0.06863873  Foreign Policy/China  4.836930e-02  \n",
       "7     Domestic Affairs*0.111396484   Scientific Research  9.918941e-01  \n",
       "8       Food/Lifestyle*0.082700245         Gender Issues  1.434017e-04  \n",
       "9             Education*0.05090855      Domestic Affairs  3.759886e-02  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(all_news, open('../data/article_set', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
